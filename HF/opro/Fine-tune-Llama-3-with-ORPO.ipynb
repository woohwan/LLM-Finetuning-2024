{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune Llama 3 with ORPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "base_model = \"meta-llama/Meta-Llama-3-8B\"\n",
    "new_model = \"OrpoLlama-3-8B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set torch dtype and attention implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set torch dtype and attention implementation\n",
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    torch_dtype = torch.bfloat16\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "    torch_dtype = torch.float16\n",
    "    attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config: QLoRA, Lora, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|██████████| 4/4 [02:59<00:00, 44.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# QLora config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Lora config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CASUAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj','gate_proj']\n",
    "\n",
    "\n",
    ")\n",
    "\n",
    "# load_tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.92k/2.92k [00:00<00:00, 5.50MB/s]\n",
      "Downloading data: 100%|██████████| 115M/115M [00:05<00:00, 20.6MB/s] \n",
      "Generating train split: 100%|██████████| 44245/44245 [00:00<00:00, 214607.35 examples/s]\n",
      "Map (num_proc=32): 100%|██████████| 1000/1000 [00:00<00:00, 4455.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000)) # Only use 1000 samples for quick demo\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= os.cpu_count(),\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opro_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    beta=0.1,\n",
    "    per_device_eval_batch_size=2,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"wandb\",\n",
    "    output_dir='./results',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whpark/miniconda3/envs/dev/lib/python3.11/site-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 990/990 [00:01<00:00, 666.64 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 613.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=opro_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['test'],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwhpark70\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/whpark/workspace/LLM-Finetuning-2024/HF/opro/wandb/run-20240430_145730-gnn4qgym</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/whpark70/huggingface/runs/gnn4qgym' target=\"_blank\">vocal-energy-2</a></strong> to <a href='https://wandb.ai/whpark70/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/whpark70/huggingface' target=\"_blank\">https://wandb.ai/whpark70/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/whpark70/huggingface/runs/gnn4qgym' target=\"_blank\">https://wandb.ai/whpark70/huggingface/runs/gnn4qgym</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whpark/miniconda3/envs/dev/lib/python3.11/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 1:41:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.909900</td>\n",
       "      <td>2.581663</td>\n",
       "      <td>19.879200</td>\n",
       "      <td>0.503000</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>-0.310095</td>\n",
       "      <td>-0.328823</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.018727</td>\n",
       "      <td>-3.288229</td>\n",
       "      <td>-3.100955</td>\n",
       "      <td>-1.316847</td>\n",
       "      <td>-1.366814</td>\n",
       "      <td>2.516560</td>\n",
       "      <td>-0.651033</td>\n",
       "      <td>0.204477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.292900</td>\n",
       "      <td>2.233691</td>\n",
       "      <td>19.857600</td>\n",
       "      <td>0.504000</td>\n",
       "      <td>0.252000</td>\n",
       "      <td>-0.264912</td>\n",
       "      <td>-0.283524</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.018612</td>\n",
       "      <td>-2.835243</td>\n",
       "      <td>-2.649124</td>\n",
       "      <td>-1.496533</td>\n",
       "      <td>-1.454357</td>\n",
       "      <td>2.169855</td>\n",
       "      <td>-0.638369</td>\n",
       "      <td>0.209496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.147300</td>\n",
       "      <td>1.975191</td>\n",
       "      <td>19.993300</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.228923</td>\n",
       "      <td>-0.242566</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.013643</td>\n",
       "      <td>-2.425660</td>\n",
       "      <td>-2.289232</td>\n",
       "      <td>-1.332117</td>\n",
       "      <td>-1.262263</td>\n",
       "      <td>1.911865</td>\n",
       "      <td>-0.633258</td>\n",
       "      <td>0.171810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.676100</td>\n",
       "      <td>1.859465</td>\n",
       "      <td>19.956900</td>\n",
       "      <td>0.501000</td>\n",
       "      <td>0.251000</td>\n",
       "      <td>-0.212653</td>\n",
       "      <td>-0.226282</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.013629</td>\n",
       "      <td>-2.262823</td>\n",
       "      <td>-2.126532</td>\n",
       "      <td>-1.251947</td>\n",
       "      <td>-1.169109</td>\n",
       "      <td>1.796515</td>\n",
       "      <td>-0.629503</td>\n",
       "      <td>0.182587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whpark/miniconda3/envs/dev/lib/python3.11/site-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
